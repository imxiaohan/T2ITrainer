# -*- coding: utf-8 -*-
"""
å¤šè¯­è¨€ç¿»è¯‘æ–‡ä»¶
åŒ…å«æ‰€æœ‰ç•Œé¢æ–‡æœ¬çš„ä¸­è‹±æ–‡ç¿»è¯‘
"""

# ç¿»è¯‘å­—å…¸ - åŒ…å«æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
TRANSLATIONS = {
    'zh': {
        'title': '## LoRA è®­ç»ƒ',
        'script': 'è®­ç»ƒè„šæœ¬',
        'config_path': 'é…ç½®æ–‡ä»¶è·¯å¾„ (.jsonæ–‡ä»¶)',
        'config_path_placeholder': 'è¾“å…¥ä¿å­˜/åŠ è½½é…ç½®çš„è·¯å¾„',
        'config_file_path': 'é…ç½®æ–‡ä»¶è·¯å¾„',
        'save': 'ä¸‹è½½é…ç½®',
        'load': 'åŠ è½½é…ç½®',
        'directory_section': 'ç›®å½•é…ç½®',
        'output_dir': 'è¾“å‡ºç›®å½•',
        'output_dir_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®',
        'save_name': 'ä¿å­˜åç§°',
        'save_name_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜åç§°',
        'pretrained_model_name_or_path': 'é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„',
        'pretrained_model_name_or_path_placeholder': 'ä»“åº“åç§°æˆ–åŒ…å«diffusersæ¨¡å‹ç»“æ„çš„ç›®å½•',
        'resume_from_checkpoint': 'ä»æ£€æŸ¥ç‚¹æ¢å¤',
        'resume_from_checkpoint_placeholder': 'ä»é€‰å®šç›®å½•æ¢å¤loraæƒé‡',
        'train_data_dir': 'è®­ç»ƒæ•°æ®ç›®å½•',
        'train_data_dir_placeholder': 'åŒ…å«æ•°æ®é›†çš„ç›®å½•',
        'model_path': 'æ¨¡å‹è·¯å¾„',
        'model_path_placeholder': 'å¦‚æœä¸æ˜¯ä»å®˜æ–¹æƒé‡è®­ç»ƒåˆ™ä¸ºå•ä¸ªæƒé‡æ–‡ä»¶',
        'report_to': 'æŠ¥å‘Šåˆ°',
        'lora_config': 'LoRA é…ç½®',
        'rank': 'ç§©',
        'rank_info': 'å»ºè®®å¯¹å°äº100çš„è®­ç»ƒé›†ä½¿ç”¨ç§©4',
        'train_batch_size': 'è®­ç»ƒæ‰¹æ¬¡å¤§å°',
        'train_batch_size_info': 'æ‰¹æ¬¡å¤§å°1ä½¿ç”¨18GBã€‚è¯·ä½¿ç”¨å°æ‰¹æ¬¡å¤§å°ä»¥é¿å…å†…å­˜ä¸è¶³',
        'repeats': 'é‡å¤æ¬¡æ•°',
        'gradient_accumulation_steps': 'æ¢¯åº¦ç´¯ç§¯æ­¥æ•°',
        'mixed_precision': 'æ··åˆç²¾åº¦',
        'gradient_checkpointing': 'æ¢¯åº¦æ£€æŸ¥ç‚¹',
        'optimizer': 'ä¼˜åŒ–å™¨',
        'lr_scheduler': 'å­¦ä¹ ç‡è°ƒåº¦å™¨',
        'cosine_restarts': 'ä½™å¼¦é‡å¯',
        'cosine_restarts_info': 'ä»…å¯¹å­¦ä¹ ç‡è°ƒåº¦å™¨cosine_with_restartsæœ‰ç”¨',
        'learning_rate': 'å­¦ä¹ ç‡',
        'learning_rate_info': 'æ¨èï¼š1e-4 æˆ– prodigyä½¿ç”¨1',
        'lr_warmup_steps': 'å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°',
        'seed': 'éšæœºç§å­',
        'blocks_to_swap': 'äº¤æ¢å—æ•°',
        'blocks_to_swap_info': 'äº¤æ¢åˆ°CPUçš„å—æ•°ã€‚å»ºè®®24GBä½¿ç”¨10ï¼Œæ›´ä½æ˜¾å­˜ä½¿ç”¨æ›´å¤š',
        'mask_dropout': 'æ©ç ä¸¢å¼ƒ',
        'mask_dropout_info': 'ä¸¢å¼ƒæ©ç ï¼Œæ„å‘³ç€æ•´ä¸ªå›¾åƒé‡å»ºçš„æ©ç å…¨ä¸º1',
        'reg_ratio': 'æ­£åˆ™åŒ–æ¯”ç‡',
        'reg_ratio_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º1',
        'reg_timestep': 'æ­£åˆ™åŒ–æ—¶é—´æ­¥',
        'reg_timestep_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º0',
        'misc': 'æ‚é¡¹',
        'num_train_epochs': 'è®­ç»ƒè½®æ•°',
        'num_train_epochs_info': 'è®­ç»ƒçš„æ€»è½®æ•°',
        'save_model_epochs': 'ä¿å­˜æ¨¡å‹è½®æ•°',
        'save_model_epochs_info': 'æ¯xè½®ä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_epochs': 'éªŒè¯è½®æ•°',
        'validation_epochs_info': 'æ¯xè½®æ‰§è¡ŒéªŒè¯',
        'skip_epoch': 'è·³è¿‡è½®æ•°',
        'skip_epoch_info': 'è·³è¿‡xè½®è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'skip_step': 'è·³è¿‡æ­¥æ•°',
        'skip_step_info': 'è·³è¿‡xæ­¥è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_ratio': 'éªŒè¯æ¯”ä¾‹',
        'validation_ratio_info': 'æŒ‰æ­¤æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†ç”¨äºéªŒè¯',
        'recreate_cache': 'é‡æ–°åˆ›å»ºç¼“å­˜',
        'caption_dropout': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'caption_dropout_info': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'max_time_steps': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        'max_time_steps_info': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        'resolution_section': '## å®éªŒé€‰é¡¹ï¼šåˆ†è¾¨ç‡\n- åŸºäºç›®æ ‡åˆ†è¾¨ç‡ï¼ˆé»˜è®¤ï¼š1024ï¼‰ã€‚\n- æ”¯æŒ512æˆ–1024ã€‚',
        'resolution': 'åˆ†è¾¨ç‡',
        'output_box': 'è¾“å‡ºæ¡†',
        'run': 'è¿è¡Œ',
        'run_button': 'è¿è¡Œ',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°English',
        'refresh': 'åˆ·æ–°',
        'upload_config_file': 'ä¸Šä¼ é…ç½®æ–‡ä»¶',
        'download_link': 'ä¸‹è½½é“¾æ¥',
    },
    'en': {
        'title': '## Lora Training',
        'script': 'script',
        'config_path': 'Config Path (.json file)',
        'config_path_placeholder': 'Enter path to save/load config',
        'config_file_path': 'Config File Path',
        'save': 'Download Config',
        'load': 'Load Config',
        'directory_section': 'Directory section',
        'output_dir': 'output_dir',
        'output_dir_placeholder': 'checkpoint save to',
        'save_name': 'save_name',
        'save_name_placeholder': 'checkpoint save name',
        'pretrained_model_name_or_path': 'pretrained_model_name_or_path',
        'pretrained_model_name_or_path_placeholder': 'repo name or dir contains diffusers model structure',
        'resume_from_checkpoint': 'resume_from_checkpoint',
        'resume_from_checkpoint_placeholder': 'resume the lora weight from seleted dir',
        'train_data_dir': 'train_data_dir',
        'train_data_dir_placeholder': 'dir contains dataset',
        'model_path': 'model_path',
        'model_path_placeholder': 'single weight files if not trained from official weight',
        'report_to': 'report_to',
        'lora_config': 'Lora Config',
        'rank': 'rank',
        'rank_info': 'Recommanded to use rank 4 for training set small than 100.',
        'train_batch_size': 'train_batch_size',
        'train_batch_size_info': 'Batch size 1 is using 18GB. Please use small batch size to avoid oom.',
        'repeats': 'repeats',
        'gradient_accumulation_steps': 'gradient_accumulation_steps',
        'mixed_precision': 'mixed_precision',
        'gradient_checkpointing': 'gradient_checkpointing',
        'optimizer': 'optimizer',
        'lr_scheduler': 'lr_scheduler',
        'cosine_restarts': 'cosine_restarts',
        'cosine_restarts_info': 'Only useful for lr_scheduler: cosine_with_restarts',
        'learning_rate': 'learning_rate',
        'learning_rate_info': 'Recommended: 1e-4 or 1 for prodigy',
        'lr_warmup_steps': 'lr_warmup_steps',
        'seed': 'seed',
        'blocks_to_swap': 'blocks_to_swap',
        'blocks_to_swap_info': 'How many blocks to swap to cpu. It is suggested 10 for 24 GB and more for lower VRAM',
        'mask_dropout': 'mask_dropout',
        'mask_dropout_info': 'Dropout mask which means mask is all one for whole image reconstruction',
        'reg_ratio': 'reg_ratio',
        'reg_ratio_info': 'As regularization of objective transfer learning. Set as 1 if you aren\'t training different objective.',
        'reg_timestep': 'reg_timestep',
        'reg_timestep_info': 'As regularization of objective transfer learning. Set as 0 if you aren\'t training different objective.',
        'misc': 'Misc',
        'num_train_epochs': 'num_train_epochs',
        'num_train_epochs_info': 'Total epoches of the training',
        'save_model_epochs': 'save_model_epochs',
        'save_model_epochs_info': 'Save checkpoint when x epoches',
        'validation_epochs': 'validation_epochs',
        'validation_epochs_info': 'perform validation when x epoches',
        'skip_epoch': 'skip_epoch',
        'skip_epoch_info': 'Skip x epoches for validation and save checkpoint',
        'skip_step': 'skip_step',
        'skip_step_info': 'Skip x steps for validation and save checkpoint',
        'validation_ratio': 'validation_ratio',
        'validation_ratio_info': 'Split dataset with this ratio for validation',
        'recreate_cache': 'recreate_cache',
        'caption_dropout': 'Caption Dropout',
        'caption_dropout_info': 'Caption Dropout',
        'max_time_steps': 'Max timesteps limitation',
        'max_time_steps_info': 'Max timesteps limitation',
        'resolution_section': '## Experiment Option: resolution\n- Based target resolution (default:1024). \n- 512 or 1024 are supported.',
        'resolution': 'resolution',
        'output_box': 'Output Box',
        'run': 'Run',
        'run_button': 'Run',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°ä¸­æ–‡',
        'refresh': 'Refresh',
        'upload_config_file': 'Upload Config File',
        'download_link': 'Download Link',
    }
}

# æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
SUPPORTED_LANGUAGES = ['zh', 'en']

# é»˜è®¤è¯­è¨€
DEFAULT_LANGUAGE = 'en'

def get_text(key, language=None):
    """
    è·å–æŒ‡å®šè¯­è¨€çš„æ–‡æœ¬
    
    Args:
        key (str): æ–‡æœ¬é”®å
        language (str, optional): è¯­è¨€ä»£ç ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è¯­è¨€
    
    Returns:
        str: å¯¹åº”çš„ç¿»è¯‘æ–‡æœ¬ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›é”®åæœ¬èº«
    """
    if language is None:
        language = DEFAULT_LANGUAGE
    
    if language not in TRANSLATIONS:
        language = DEFAULT_LANGUAGE
    
    return TRANSLATIONS[language].get(key, key)

def get_supported_languages():
    """
    è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
    
    Returns:
        list: æ”¯æŒçš„è¯­è¨€ä»£ç åˆ—è¡¨
    """
    return SUPPORTED_LANGUAGES.copy()

def is_supported_language(language):
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è¯­è¨€
    
    Args:
        language (str): è¯­è¨€ä»£ç 
    
    Returns:
        bool: æ˜¯å¦ä¸ºæ”¯æŒçš„è¯­è¨€
    """
    return language in SUPPORTED_LANGUAGES 