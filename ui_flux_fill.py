import gradio as gr

import subprocess
import json
import sys
import os

css_code = """
.my-file-upload {
    height: 60px !important;
}
.my-file-upload .download-link {
    display: none !important;
}
.spacing-section {
    margin-bottom: 20px !important;
    padding-bottom: 20px !important;
    border-bottom: 2px dotted #e0e0e0 !important;
}
/* è®¾ç½®é¡µé¢æ•´ä½“å®½åº¦ */
.gradio-container {
    max-width: 960px !important;
    width: 100% !important;
    margin: 0 auto !important;
}
"""

# ===== è¯­è¨€ç¿»è¯‘ç³»ç»Ÿ =====
# ç¿»è¯‘å­—å…¸ - åŒ…å«æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
TRANSLATIONS = {
    'zh': {
        'title': '## LoRA è®­ç»ƒ',
        'script': 'è®­ç»ƒè„šæœ¬',
        'config_path': 'é…ç½®æ–‡ä»¶è·¯å¾„ (.jsonæ–‡ä»¶)',
        'config_path_placeholder': 'è¾“å…¥ä¿å­˜/åŠ è½½é…ç½®çš„è·¯å¾„',
        'save': 'ä¸‹è½½é…ç½®',
        'load': 'åŠ è½½é…ç½®',
        'directory_section': 'ç›®å½•é…ç½®',
        'output_dir': 'è¾“å‡ºç›®å½•',
        'output_dir_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®',
        'save_name': 'ä¿å­˜åç§°',
        'save_name_placeholder': 'æ£€æŸ¥ç‚¹ä¿å­˜åç§°',
        'pretrained_model_name_or_path': 'é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–è·¯å¾„',
        'pretrained_model_placeholder': 'ä»“åº“åç§°æˆ–åŒ…å«diffusersæ¨¡å‹ç»“æ„çš„ç›®å½•',
        'resume_from_checkpoint': 'ä»æ£€æŸ¥ç‚¹æ¢å¤',
        'resume_checkpoint_placeholder': 'ä»é€‰å®šç›®å½•æ¢å¤loraæƒé‡',
        'train_data_dir': 'è®­ç»ƒæ•°æ®ç›®å½•',
        'train_data_dir_placeholder': 'åŒ…å«æ•°æ®é›†çš„ç›®å½•',
        'model_path': 'æ¨¡å‹è·¯å¾„',
        'model_path_placeholder': 'å¦‚æœä¸æ˜¯ä»å®˜æ–¹æƒé‡è®­ç»ƒåˆ™ä¸ºå•ä¸ªæƒé‡æ–‡ä»¶',
        'report_to': 'æŠ¥å‘Šåˆ°',
        'lora_config': 'LoRA é…ç½®',
        'rank': 'ç§©',
        'rank_info': 'å»ºè®®å¯¹å°äº100çš„è®­ç»ƒé›†ä½¿ç”¨ç§©4',
        'train_batch_size': 'è®­ç»ƒæ‰¹æ¬¡å¤§å°',
        'batch_size_info': 'æ‰¹æ¬¡å¤§å°1ä½¿ç”¨18GBã€‚è¯·ä½¿ç”¨å°æ‰¹æ¬¡å¤§å°ä»¥é¿å…å†…å­˜ä¸è¶³',
        'repeats': 'é‡å¤æ¬¡æ•°',
        'gradient_accumulation_steps': 'æ¢¯åº¦ç´¯ç§¯æ­¥æ•°',
        'mixed_precision': 'æ··åˆç²¾åº¦',
        'gradient_checkpointing': 'æ¢¯åº¦æ£€æŸ¥ç‚¹',
        'optimizer': 'ä¼˜åŒ–å™¨',
        'lr_scheduler': 'å­¦ä¹ ç‡è°ƒåº¦å™¨',
        'cosine_restarts': 'ä½™å¼¦é‡å¯',
        'cosine_restarts_info': 'ä»…å¯¹å­¦ä¹ ç‡è°ƒåº¦å™¨cosine_with_restartsæœ‰ç”¨',
        'learning_rate': 'å­¦ä¹ ç‡',
        'learning_rate_info': 'æ¨èï¼š1e-4 æˆ– prodigyä½¿ç”¨1',
        'lr_warmup_steps': 'å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°',
        'seed': 'éšæœºç§å­',
        'blocks_to_swap': 'äº¤æ¢å—æ•°',
        'blocks_to_swap_info': 'äº¤æ¢åˆ°CPUçš„å—æ•°ã€‚å»ºè®®24GBä½¿ç”¨10ï¼Œæ›´ä½æ˜¾å­˜ä½¿ç”¨æ›´å¤š',
        'mask_dropout': 'æ©ç ä¸¢å¼ƒ',
        'mask_dropout_info': 'ä¸¢å¼ƒæ©ç ï¼Œæ„å‘³ç€æ•´ä¸ªå›¾åƒé‡å»ºçš„æ©ç å…¨ä¸º1',
        'reg_ratio': 'æ­£åˆ™åŒ–æ¯”ç‡',
        'reg_ratio_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º1',
        'reg_timestep': 'æ­£åˆ™åŒ–æ—¶é—´æ­¥',
        'reg_timestep_info': 'ä½œä¸ºç›®æ ‡è¿ç§»å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å¦‚æœä¸è®­ç»ƒä¸åŒç›®æ ‡åˆ™è®¾ä¸º0',
        'misc': 'æ‚é¡¹',
        'num_train_epochs': 'è®­ç»ƒè½®æ•°',
        'num_train_epochs_info': 'è®­ç»ƒçš„æ€»è½®æ•°',
        'save_model_epochs': 'ä¿å­˜æ¨¡å‹è½®æ•°',
        'save_model_epochs_info': 'æ¯xè½®ä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_epochs': 'éªŒè¯è½®æ•°',
        'validation_epochs_info': 'æ¯xè½®æ‰§è¡ŒéªŒè¯',
        'skip_epoch': 'è·³è¿‡è½®æ•°',
        'skip_epoch_info': 'è·³è¿‡xè½®è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'skip_step': 'è·³è¿‡æ­¥æ•°',
        'skip_step_info': 'è·³è¿‡xæ­¥è¿›è¡ŒéªŒè¯å’Œä¿å­˜æ£€æŸ¥ç‚¹',
        'validation_ratio': 'éªŒè¯æ¯”ä¾‹',
        'validation_ratio_info': 'æŒ‰æ­¤æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†ç”¨äºéªŒè¯',
        'recreate_cache': 'é‡æ–°åˆ›å»ºç¼“å­˜',
        'caption_dropout': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'caption_dropout_info': 'æ ‡é¢˜ä¸¢å¼ƒ',
        'max_time_steps': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        'max_time_steps_info': 'æœ€å¤§æ—¶é—´æ­¥é™åˆ¶',
        'resolution_section': '## å®éªŒé€‰é¡¹ï¼šåˆ†è¾¨ç‡\n- åŸºäºç›®æ ‡åˆ†è¾¨ç‡ï¼ˆé»˜è®¤ï¼š1024ï¼‰ã€‚\n- æ”¯æŒ512æˆ–1024ã€‚',
        'resolution': 'åˆ†è¾¨ç‡',
        'output_box': 'è¾“å‡ºæ¡†',
        'run': 'è¿è¡Œ',
        'run_button': 'è¿è¡Œ',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°English',
        'refresh': 'åˆ·æ–°',
        'upload_config_file': 'ä¸Šä¼ é…ç½®æ–‡ä»¶',
        'config_file_path': 'é…ç½®æ–‡ä»¶è·¯å¾„',
        'download_link': 'ä¸‹è½½é“¾æ¥',
    },
    'en': {
        'title': '## Lora Training',
        'script': 'script',
        'config_path': 'Config Path (.json file)',
        'config_path_placeholder': 'Enter path to save/load config',
        'save': 'Download Config',
        'load': 'Load Config',
        'directory_section': 'Directory section',
        'output_dir': 'output_dir',
        'output_dir_placeholder': 'checkpoint save to',
        'save_name': 'save_name',
        'save_name_placeholder': 'checkpoint save name',
        'pretrained_model_name_or_path': 'pretrained_model_name_or_path',
        'pretrained_model_placeholder': 'repo name or dir contains diffusers model structure',
        'resume_from_checkpoint': 'resume_from_checkpoint',
        'resume_checkpoint_placeholder': 'resume the lora weight from seleted dir',
        'train_data_dir': 'train_data_dir',
        'train_data_dir_placeholder': 'dir contains dataset',
        'model_path': 'model_path',
        'model_path_placeholder': 'single weight files if not trained from official weight',
        'report_to': 'report_to',
        'lora_config': 'Lora Config',
        'rank': 'rank',
        'rank_info': 'Recommanded to use rank 4 for training set small than 100.',
        'train_batch_size': 'train_batch_size',
        'batch_size_info': 'Batch size 1 is using 18GB. Please use small batch size to avoid oom.',
        'repeats': 'repeats',
        'gradient_accumulation_steps': 'gradient_accumulation_steps',
        'mixed_precision': 'mixed_precision',
        'gradient_checkpointing': 'gradient_checkpointing',
        'optimizer': 'optimizer',
        'lr_scheduler': 'lr_scheduler',
        'cosine_restarts': 'cosine_restarts',
        'cosine_restarts_info': 'Only useful for lr_scheduler: cosine_with_restarts',
        'learning_rate': 'learning_rate',
        'learning_rate_info': 'Recommended: 1e-4 or 1 for prodigy',
        'lr_warmup_steps': 'lr_warmup_steps',
        'seed': 'seed',
        'blocks_to_swap': 'blocks_to_swap',
        'blocks_to_swap_info': 'How many blocks to swap to cpu. It is suggested 10 for 24 GB and more for lower VRAM',
        'mask_dropout': 'mask_dropout',
        'mask_dropout_info': 'Dropout mask which means mask is all one for whole image reconstruction',
        'reg_ratio': 'reg_ratio',
        'reg_ratio_info': 'As regularization of objective transfer learning. Set as 1 if you aren\'t training different objective.',
        'reg_timestep': 'reg_timestep',
        'reg_timestep_info': 'As regularization of objective transfer learning. Set as 0 if you aren\'t training different objective.',
        'misc': 'Misc',
        'num_train_epochs': 'num_train_epochs',
        'num_train_epochs_info': 'Total epoches of the training',
        'save_model_epochs': 'save_model_epochs',
        'save_model_epochs_info': 'Save checkpoint when x epoches',
        'validation_epochs': 'validation_epochs',
        'validation_epochs_info': 'perform validation when x epoches',
        'skip_epoch': 'skip_epoch',
        'skip_epoch_info': 'Skip x epoches for validation and save checkpoint',
        'skip_step': 'skip_step',
        'skip_step_info': 'Skip x steps for validation and save checkpoint',
        'validation_ratio': 'validation_ratio',
        'validation_ratio_info': 'Split dataset with this ratio for validation',
        'recreate_cache': 'recreate_cache',
        'caption_dropout': 'Caption Dropout',
        'caption_dropout_info': 'Caption Dropout',
        'max_time_steps': 'Max timesteps limitation',
        'max_time_steps_info': 'Max timesteps limitation',
        'resolution_section': '## Experiment Option: resolution\n- Based target resolution (default:1024). \n- 512 or 1024 are supported.',
        'resolution': 'resolution',
        'output_box': 'Output Box',
        'run': 'Run',
        'run_button': 'Run',
        'language_toggle': 'ğŸŒ åˆ‡æ¢åˆ°ä¸­æ–‡',
        'refresh': 'Refresh',
        'upload_config_file': 'Upload Config File',
        'config_file_path': 'Config File Path',
        'download_link': 'Download Link',
    }
}

# å½“å‰è¯­è¨€çŠ¶æ€
current_language = 'en'

# ä»é…ç½®æ–‡ä»¶åŠ è½½é»˜è®¤é…ç½®
def load_default_config():
    """ä»default_config.jsonåŠ è½½é»˜è®¤é…ç½®"""
    try:
        with open("default_config.json", 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading default config: {e}")
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿”å›ç©ºé…ç½®
        return {
            "script": "train_flux_lora_ui_kontext.py",
            "script_choices": [
                "train_flux_lora_ui_kontext.py",
                "train_flux_lora_ui_with_mask.py",
                "train_flux_lora_ui.py"
            ],
            "config_path": "default_config.json"
        }

# åŠ è½½é»˜è®¤é…ç½®
default_config = load_default_config()



# é¡µé¢åˆå§‹åŒ–é…ç½® - ä½¿ç”¨é»˜è®¤é…ç½®
page_config = default_config

# è·å–å½“å‰è¯­è¨€çš„æ–‡æœ¬
def get_text(key):
    """è·å–å½“å‰è¯­è¨€çš„æ–‡æœ¬"""
    return TRANSLATIONS[current_language].get(key, key)

def toggle_language():
    """åˆ‡æ¢è¯­è¨€"""
    global current_language
    current_language = 'en' if current_language == 'zh' else 'zh'
    return current_language

# ä¿å­˜é…ç½®
def save_config( 
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
        # use_fp8
        # freeze_transformer_layers
    ):
    # Ensure config_path is just filename, save to configs directory
    configs_dir = os.path.join(os.getcwd(), "configs")
    os.makedirs(configs_dir, exist_ok=True)
    
    # Handle config_path which might be a relative path
    if config_path:
        # If config_path contains directory separators, it's a relative path
        if '/' in config_path or '\\' in config_path:
            # Use the relative path as is
            filename = config_path
        else:
            # If just filename provided
            filename = config_path
    else:
        filename = "config.json"
    
    # Ensure .json extension
    if not filename.endswith('.json'):
        filename += '.json'
    
    # Create full path in configs directory
    full_config_path = os.path.join(configs_dir, filename)
    
    config = {
        "script":script,
        "seed":seed,
        # "logging_dir":logging_dir,
        "mixed_precision":mixed_precision,
        "report_to":report_to,
        "lr_warmup_steps":lr_warmup_steps,
        "output_dir":output_dir,
        "save_name":save_name,
        "train_data_dir":train_data_dir,
        "optimizer":optimizer,
        "lr_scheduler":lr_scheduler,
        "learning_rate":learning_rate,
        "train_batch_size":train_batch_size,
        "repeats":repeats,
        "gradient_accumulation_steps":gradient_accumulation_steps,
        "num_train_epochs":num_train_epochs,
        "save_model_epochs":save_model_epochs,
        "validation_epochs":validation_epochs,
        "rank":rank,
        "skip_epoch":skip_epoch,
        # "break_epoch":break_epoch,
        "skip_step":skip_step,
        "gradient_checkpointing":gradient_checkpointing,
        "validation_ratio":validation_ratio,
        "pretrained_model_name_or_path":pretrained_model_name_or_path,
        "model_path":model_path,
        "resume_from_checkpoint":resume_from_checkpoint,
        # "use_dora":use_dora,
        "recreate_cache":recreate_cache,
        # "vae_path":vae_path,
        "config_path":filename,  # Store relative filename
        "resolution":resolution,
        # "use_debias":use_debias,
        # 'snr_gamma':snr_gamma,
        "caption_dropout":caption_dropout,
        "cosine_restarts":cosine_restarts,
        "max_time_steps":max_time_steps,
        # "freeze_transformer_layers":freeze_transformer_layers
        "blocks_to_swap":blocks_to_swap,
        "mask_dropout":mask_dropout,
        "reg_ratio":reg_ratio,
        "reg_timestep":reg_timestep
        # "use_fp8":use_fp8
    }
    
    # Save to configs directory
    with open(full_config_path, 'w') as f:
        json.dump(config, f, indent=4)
    print(f"Configuration saved to {full_config_path}")
    
    # Also save a copy as default config.json for backward compatibility
    with open("config.json", 'w') as f:
        json.dump(config, f, indent=4)

# è·å–é…ç½®æ–‡ä»¶åˆ—è¡¨
def get_config_files():
    """è·å–configsç›®å½•ä¸­æ‰€æœ‰JSONæ–‡ä»¶çš„åˆ—è¡¨ï¼ˆåŒ…æ‹¬å­ç›®å½•ï¼‰"""
    configs_dir = os.path.join(os.getcwd(), "configs")
    if not os.path.exists(configs_dir):
        return ["config.json"]
    
    json_files = []
    
    # é€’å½’æœç´¢æ‰€æœ‰å­ç›®å½•
    for root, dirs, files in os.walk(configs_dir):
        for file in files:
            if file.endswith('.json'):
                # è®¡ç®—ç›¸å¯¹è·¯å¾„
                rel_path = os.path.relpath(os.path.join(root, file), configs_dir)
                json_files.append(rel_path)
    
    return sorted(json_files) if json_files else ["config.json"]

# Function to load configuration from a file path
def load_config_from_file(config_path_input):
    """
    ä»æ–‡ä»¶åŠ è½½é…ç½®
    config_path_input: é…ç½®æ–‡ä»¶è·¯å¾„è¾“å…¥æ¡†çš„å€¼
    """
    if config_path_input is None or config_path_input.strip() == "":
        print("No config path provided")
        return None
    
    config_path = config_path_input.strip()
    
    # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œå°è¯•ä»å½“å‰ç›®å½•æˆ–configsç›®å½•åŠ è½½
    if not os.path.isabs(config_path):
        # å…ˆå°è¯•å½“å‰ç›®å½•
        if os.path.exists(config_path):
            config_path = config_path
        else:
            # å†å°è¯•configsç›®å½•
            configs_dir = os.path.join(os.getcwd(), "configs")
            configs_path = os.path.join(configs_dir, config_path)
            if os.path.exists(configs_path):
                config_path = configs_path
            else:
                print(f"Config file not found: {config_path}")
                return None
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"Loaded configuration from {config_path}")
    except Exception as e:
        print(f"Error loading config: {e}")
        return None
    
    # è¿”å›åŠ è½½çš„é…ç½®å€¼
    return [
        config_path_input,
        config.get('script', page_config['script']),
        config.get('seed', page_config['seed']),
        config.get('mixed_precision', page_config['mixed_precision']),
        config.get('report_to', page_config['report_to']),
        config.get('lr_warmup_steps', page_config['lr_warmup_steps']),
        config.get('output_dir', page_config['output_dir']),
        config.get('save_name', page_config['save_name']),
        config.get('train_data_dir', page_config['train_data_dir']),
        config.get('optimizer', page_config['optimizer']),
        config.get('lr_scheduler', page_config['lr_scheduler']),
        config.get('learning_rate', page_config['learning_rate']),
        config.get('train_batch_size', page_config['train_batch_size']),
        config.get('repeats', page_config['repeats']),
        config.get('gradient_accumulation_steps', page_config['gradient_accumulation_steps']),
        config.get('num_train_epochs', page_config['num_train_epochs']),
        config.get('save_model_epochs', page_config['save_model_epochs']),
        config.get('validation_epochs', page_config['validation_epochs']),
        config.get('rank', page_config['rank']),
        config.get('skip_epoch', page_config['skip_epoch']),
        config.get('skip_step', page_config['skip_step']),
        config.get('gradient_checkpointing', page_config['gradient_checkpointing']),
        config.get('validation_ratio', page_config['validation_ratio']),
        config.get('pretrained_model_name_or_path', page_config['pretrained_model_name_or_path']),
        config.get('model_path', page_config['model_path']),
        config.get('resume_from_checkpoint', page_config['resume_from_checkpoint']),
        config.get('recreate_cache', page_config['recreate_cache']),
        config.get('resolution', page_config['resolution']),
        config.get('caption_dropout', page_config['caption_dropout']),
        config.get('cosine_restarts', page_config['cosine_restarts']),
        config.get('max_time_steps', page_config['max_time_steps']),
        config.get('blocks_to_swap', page_config['blocks_to_swap']),
        config.get('mask_dropout', page_config['mask_dropout']),
        config.get('reg_ratio', page_config['reg_ratio']),
        config.get('reg_timestep', page_config['reg_timestep'])
    ]
        # default_config['use_dora'], \
        # default_config['freeze_transformer_layers']
        # default_config['logging_dir'],default_config['break_epoch'], 

# ä¸‹è½½é…ç½®æ–‡ä»¶åŠŸèƒ½
def download_config_file(config_path_input):
    """
    ä¸‹è½½é…ç½®æ–‡ä»¶
    config_path_input: é…ç½®æ–‡ä»¶è·¯å¾„è¾“å…¥æ¡†çš„å€¼
    """
    if config_path_input is None or config_path_input.strip() == "":
        print("No config path provided")
        return None
    
    config_path = config_path_input.strip()
    
    # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œå°è¯•ä»å½“å‰ç›®å½•æˆ–configsç›®å½•åŠ è½½
    if not os.path.isabs(config_path):
        # å…ˆå°è¯•å½“å‰ç›®å½•
        if os.path.exists(config_path):
            config_path = config_path
        else:
            # å†å°è¯•configsç›®å½•
            configs_dir = os.path.join(os.getcwd(), "configs")
            configs_path = os.path.join(configs_dir, config_path)
            if os.path.exists(configs_path):
                config_path = configs_path
            else:
                print(f"Config file not found: {config_path}")
                return None
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        print(f"Config file not found: {config_path}")
        return None
    
    # è¿”å›æ–‡ä»¶è·¯å¾„ä¾›ä¸‹è½½
    print(f"Preparing to download: {config_path}")
    return config_path

# ä¸‹è½½é…ç½®æ–‡ä»¶åŠŸèƒ½ï¼ˆç”¨äºæŒ‰é’®ç‚¹å‡»ï¼‰
def download_config_file_for_button(config_path_input):
    """
    ä¸‹è½½é…ç½®æ–‡ä»¶
    config_path_input: é…ç½®æ–‡ä»¶è·¯å¾„è¾“å…¥æ¡†çš„å€¼
    """
    if config_path_input is None or config_path_input.strip() == "":
        return gr.update(value=None, visible=False)
    
    config_path = config_path_input.strip()
    
    # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œå°è¯•ä»å½“å‰ç›®å½•æˆ–configsç›®å½•åŠ è½½
    if not os.path.isabs(config_path):
        # å…ˆå°è¯•å½“å‰ç›®å½•
        if os.path.exists(config_path):
            config_path = config_path
        else:
            # å†å°è¯•configsç›®å½•
            configs_dir = os.path.join(os.getcwd(), "configs")
            configs_path = os.path.join(configs_dir, config_path)
            if os.path.exists(configs_path):
                config_path = configs_path
            else:
                return gr.update(value=None, visible=False)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        return gr.update(value=None, visible=False)
    
    # è¿”å›æ–‡ä»¶è·¯å¾„å¹¶æ˜¾ç¤ºä¸‹è½½é“¾æ¥
    print(f"Preparing to download: {config_path}")
    return gr.update(value=config_path, visible=True)

# æ³¨é‡Šæ‰è‡ªåŠ¨åŠ è½½é…ç½®åŠŸèƒ½
# load_config_from_file(None)

def run(
        config_path,
        script,
        seed,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        recreate_cache,
        resolution,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
    ):
    # Save the current configuration to the specified config file
    save_config(
        config_path,
        script,
        seed,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        recreate_cache,
        resolution,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
    )

    # Construct the command to run the script with only the config path
    command_args = [sys.executable, script, "--config_path", config_path]

    # Execute the command
    subprocess.call(command_args)

    # Return the executed command as a string
    return " ".join(command_args)

def toggle_language_handler():
    """è¯­è¨€åˆ‡æ¢å¤„ç†å‡½æ•°"""
    toggle_language()
    # è¿”å›æ›´æ–°åçš„ç»„ä»¶
    updates = []
    # æ›´æ–°æ ‡é¢˜
    updates.append(gr.Markdown(get_text('title')))
    # æ›´æ–°è¯­è¨€åˆ‡æ¢æŒ‰é’®æ–‡æœ¬
    updates.append(gr.Button(get_text('language_toggle'), scale=0, size="sm"))
    return updates

def update_language_interface():
    """æ›´æ–°ç•Œé¢è¯­è¨€ï¼Œè¿”å›æ‰€æœ‰éœ€è¦æ›´æ–°çš„ç»„ä»¶"""
    toggle_language()
    # è¿”å›æ›´æ–°åçš„æ‰€æœ‰UIç»„ä»¶
    updated_components = [
        # åŸºç¡€ç»„ä»¶
        gr.Markdown(get_text('title')),  # æ ‡é¢˜
        gr.Button(get_text('refresh'), scale=0, size="sm", min_width=50),  # åˆ·æ–°æŒ‰é’®
        gr.Button(get_text('language_toggle'), scale=0, size="sm"),  # è¯­è¨€åˆ‡æ¢æŒ‰é’®
        gr.Dropdown(label=get_text('script'), value=page_config["script"], choices=page_config["script_choices"]),  # è„šæœ¬é€‰æ‹©
        gr.Textbox(label=get_text('config_path'), value=page_config["config_path"]),  # é…ç½®è·¯å¾„
        gr.Button(get_text('load'), scale=1, variant="primary"),  # åŠ è½½æŒ‰é’®
        gr.Button(get_text('save'), scale=1, variant="secondary"),  # ä¸‹è½½æŒ‰é’®
        gr.File(label=get_text('download_link'), visible=False),  # ä¸‹è½½é“¾æ¥ç»„ä»¶
        # ç›®å½•è®¾ç½®éƒ¨åˆ†çš„ç»„ä»¶
        gr.Textbox(label=get_text('output_dir'), value=page_config["output_dir"], placeholder=get_text('output_dir_placeholder')),  # è¾“å‡ºç›®å½•
        gr.Textbox(label=get_text('save_name'), value=page_config["save_name"], placeholder=get_text('save_name_placeholder')),  # ä¿å­˜åç§°
        gr.Textbox(label=get_text('pretrained_model_name_or_path'), value=page_config["pretrained_model_name_or_path"], placeholder=get_text('pretrained_model_name_or_path_placeholder')),  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        gr.Textbox(label=get_text('resume_from_checkpoint'), value=page_config["resume_from_checkpoint"], placeholder=get_text('resume_from_checkpoint_placeholder')),  # æ¢å¤æ£€æŸ¥ç‚¹
        gr.Textbox(label=get_text('train_data_dir'), value=page_config["train_data_dir"], placeholder=get_text('train_data_dir_placeholder')),  # è®­ç»ƒæ•°æ®ç›®å½•
        gr.Textbox(label=get_text('model_path'), value=page_config["model_path"], placeholder=get_text('model_path_placeholder')),  # æ¨¡å‹è·¯å¾„
        gr.Dropdown(label=get_text('report_to'), value=page_config["report_to"], choices=["all","wandb","tensorboard"]),  # æŠ¥å‘Šåˆ°
        # LoRAé…ç½®éƒ¨åˆ†çš„ç»„ä»¶
        gr.Number(label=get_text('rank'), value=page_config["rank"], info=get_text('rank_info')),  # æ’å
        gr.Number(label=get_text('train_batch_size'), value=page_config["train_batch_size"], info=get_text('train_batch_size_info')),  # è®­ç»ƒæ‰¹æ¬¡å¤§å°
        gr.Number(label=get_text('repeats'), value=page_config["repeats"]),  # é‡å¤æ¬¡æ•°
        gr.Number(label=get_text('gradient_accumulation_steps'), value=page_config["gradient_accumulation_steps"]),  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        gr.Radio(label=get_text('mixed_precision'), value=page_config["mixed_precision"], choices=["bf16", "fp8"]),  # æ··åˆç²¾åº¦
        gr.Checkbox(label=get_text('gradient_checkpointing'), value=page_config["gradient_checkpointing"]),  # æ¢¯åº¦æ£€æŸ¥ç‚¹
        gr.Dropdown(label=get_text('optimizer'), value=page_config["optimizer"], choices=["adamw","prodigy"]),  # ä¼˜åŒ–å™¨
        gr.Dropdown(label=get_text('lr_scheduler'), value=page_config["lr_scheduler"], choices=["linear", "cosine", "cosine_with_restarts", "polynomial","constant", "constant_with_warmup"]),  # å­¦ä¹ ç‡è°ƒåº¦å™¨
        gr.Number(label=get_text('cosine_restarts'), value=page_config["cosine_restarts"], info=get_text('cosine_restarts_info'), minimum=1),  # ä½™å¼¦é‡å¯
        gr.Number(label=get_text('learning_rate'), value=page_config["learning_rate"], info=get_text('learning_rate_info')),  # å­¦ä¹ ç‡
        gr.Number(label=get_text('lr_warmup_steps'), value=page_config["lr_warmup_steps"]),  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
        gr.Number(label=get_text('seed'), value=page_config["seed"]),  # éšæœºç§å­
        gr.Number(label=get_text('blocks_to_swap'), value=page_config["blocks_to_swap"], info=get_text('blocks_to_swap_info')),  # äº¤æ¢å—æ•°
        gr.Number(label=get_text('mask_dropout'), value=page_config["mask_dropout"], info=get_text('mask_dropout_info')),  # æ©ç ä¸¢å¼ƒ
        gr.Number(label=get_text('reg_ratio'), value=page_config["reg_ratio"], info=get_text('reg_ratio_info')),  # æ­£åˆ™åŒ–æ¯”ç‡
        gr.Number(label=get_text('reg_timestep'), value=page_config["reg_timestep"], info=get_text('reg_timestep_info')),  # æ­£åˆ™åŒ–æ—¶é—´æ­¥
        # Miscéƒ¨åˆ†çš„ç»„ä»¶
        gr.Number(label=get_text('num_train_epochs'), value=page_config["num_train_epochs"], info=get_text('num_train_epochs_info')),  # è®­ç»ƒè½®æ•°
        gr.Number(label=get_text('save_model_epochs'), value=page_config["save_model_epochs"], info=get_text('save_model_epochs_info')),  # ä¿å­˜æ¨¡å‹è½®æ•°
        gr.Number(label=get_text('validation_epochs'), value=page_config["validation_epochs"], info=get_text('validation_epochs_info')),  # éªŒè¯è½®æ•°
        gr.Number(label=get_text('skip_epoch'), value=page_config["skip_epoch"], info=get_text('skip_epoch_info')),  # è·³è¿‡è½®æ•°
        gr.Number(label=get_text('skip_step'), value=page_config["skip_step"], info=get_text('skip_step_info')),  # è·³è¿‡æ­¥æ•°
        gr.Number(label=get_text('validation_ratio'), value=page_config["validation_ratio"], info=get_text('validation_ratio_info')),  # éªŒè¯æ¯”ç‡
        gr.Checkbox(label=get_text('recreate_cache'), value=page_config["recreate_cache"]),  # é‡å»ºç¼“å­˜
        gr.Number(label=get_text('caption_dropout'), value=page_config["caption_dropout"], info=get_text('caption_dropout_info'), maximum=1, minimum=0),  # æ ‡é¢˜ä¸¢å¼ƒ
        gr.Number(label=get_text('max_time_steps'), value=page_config["max_time_steps"], info=get_text('max_time_steps_info'), maximum=1000, minimum=0),  # æœ€å¤§æ—¶é—´æ­¥
        gr.Markdown(get_text('resolution_section')),  # åˆ†è¾¨ç‡è¯´æ˜
        gr.Dropdown(label=get_text('resolution'), value=page_config["resolution"], choices=page_config["resolution_choices"]),  # åˆ†è¾¨ç‡
        # è¾“å‡ºå’Œè¿è¡ŒæŒ‰é’®
        gr.Textbox(label=get_text('output_box')),  # è¾“å‡ºæ¡†
        gr.Button(get_text('run_button'))  # è¿è¡ŒæŒ‰é’®
    ]
    return updated_components
    
# å¤„ç†æ–‡ä»¶ä¸Šä¼ äº‹ä»¶
def handle_file(file_obj):
    if file_obj is None:
        return ""
    
    # åˆ›å»ºconfigsç›®å½•
    configs_dir = os.path.join(os.getcwd(), "configs")
    os.makedirs(configs_dir, exist_ok=True)
    
    # æŒ‰æ—¥æœŸåˆ›å»ºå­ç›®å½• (æ ¼å¼: YYYY-MM-DD)
    from datetime import datetime
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(configs_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    
    # è·å–åŸå§‹æ–‡ä»¶å
    original_filename = os.path.basename(file_obj.name)
    if not original_filename.endswith(".json"):
        original_filename += ".json"
    
    # æ·»åŠ æ—¶é—´æˆ³é¿å…æ–‡ä»¶åå†²çª
    timestamp = datetime.now().strftime("%H%M%S")
    name_without_ext = os.path.splitext(original_filename)[0]
    new_filename = f"{name_without_ext}_{timestamp}.json"
    
    # æ„å»ºç›®æ ‡è·¯å¾„
    target_path = os.path.join(date_dir, new_filename)
    
    try:
        # å¤åˆ¶æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
        import shutil
        shutil.copy2(file_obj.name, target_path)
        print(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {target_path}")
        
        # è¿”å›ç›¸å¯¹è·¯å¾„ (ç›¸å¯¹äºconfigsç›®å½•)
        relative_path = os.path.join(date_str, new_filename)
        return relative_path
        
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        # å¦‚æœä¿å­˜å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡ä»¶è·¯å¾„
        return file_obj.name

# åˆ›å»ºUIç•Œé¢
with gr.Blocks(css=css_code) as demo:
    # è¯­è¨€åˆ‡æ¢æŒ‰é’®å’Œåˆ·æ–°æŒ‰é’®
    with gr.Row():
        gr.HTML("<div style='flex-grow: 1;'></div>")  # å ä½ç¬¦ï¼Œè®©æŒ‰é’®å³å¯¹é½
        refresh_config_btn = gr.Button(get_text('refresh'), scale=0, size="sm", min_width=50)
        language_toggle_btn = gr.Button(get_text('language_toggle'), scale=0, size="sm", min_width=100)
    
    # æ ‡é¢˜
    title_md = gr.Markdown(get_text('title'))

     # ä¸Šä¼ é…ç½®æ–‡ä»¶
    config_file_upload = gr.File(label=get_text('upload_config_file'), file_types=[".json"], file_count="single", scale=1, elem_classes="my-file-upload")
    config_path = gr.Textbox(label=get_text('config_file_path'), value=page_config["config_path"], placeholder="è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„æˆ–ä¸Šä¼ æ–‡ä»¶")

    # é…ç½®æ–‡ä»¶æ“ä½œæŒ‰é’®
    with gr.Row(equal_height=True):
        load_config_btn = gr.Button(get_text('load'), scale=1, variant="primary")
        download_btn = gr.Button(get_text('save'), scale=1, variant="secondary")
    
    download_file = gr.File(label=get_text('download_link'), visible=False)

    gr.HTML("<div style='border-bottom: 2px dotted #e0e0e0; flex-grow: 1;'></div>")

    # è„šæœ¬é€‰æ‹©
    script = gr.Dropdown(label=get_text('script'), value=page_config["script"], choices=page_config["script_choices"])
    
    directory_accordion = gr.Accordion(get_text('directory_section'))
    with directory_accordion:
        # ç›®å½•è®¾ç½®éƒ¨åˆ†
        with gr.Row():
            output_dir = gr.Textbox(label=get_text('output_dir'), value=page_config["output_dir"],
                                   placeholder=get_text('output_dir_placeholder'))
            save_name = gr.Textbox(label=get_text('save_name'), value=page_config["save_name"],
                                   placeholder=get_text('save_name_placeholder'))
        with gr.Row():
            pretrained_model_name_or_path = gr.Textbox(label=get_text('pretrained_model_name_or_path'), 
                value=page_config["pretrained_model_name_or_path"], 
                placeholder=get_text('pretrained_model_name_or_path_placeholder')
            )
            resume_from_checkpoint = gr.Textbox(label=get_text('resume_from_checkpoint'), value=page_config["resume_from_checkpoint"], placeholder=get_text('resume_from_checkpoint_placeholder'))
        with gr.Row():
            train_data_dir = gr.Textbox(label=get_text('train_data_dir'), value=page_config["train_data_dir"], placeholder=get_text('train_data_dir_placeholder'))
            model_path = gr.Textbox(label=get_text('model_path'), value=page_config["model_path"], placeholder=get_text('model_path_placeholder'))
        with gr.Row():
            report_to = gr.Dropdown(label=get_text('report_to'), value=page_config["report_to"], choices=["all","wandb","tensorboard"])

    lora_accordion = gr.Accordion(get_text('lora_config'))
    with lora_accordion:
        # è®­ç»ƒç›¸å…³è®¾ç½®
        with gr.Row():
            rank = gr.Number(label=get_text('rank'), value=page_config["rank"], info=get_text('rank_info'))
            train_batch_size = gr.Number(label=get_text('train_batch_size'), value=page_config["train_batch_size"], info=get_text('train_batch_size_info'))
        with gr.Row():
            repeats = gr.Number(label=get_text('repeats'), value=page_config["repeats"])
            gradient_accumulation_steps = gr.Number(label=get_text('gradient_accumulation_steps'), value=page_config["gradient_accumulation_steps"])
            mixed_precision = gr.Radio(label=get_text('mixed_precision'), value=page_config["mixed_precision"], choices=["bf16", "fp8"])
            gradient_checkpointing = gr.Checkbox(label=get_text('gradient_checkpointing'), value=page_config["gradient_checkpointing"])
        with gr.Row():
            optimizer = gr.Dropdown(label=get_text('optimizer'), value=page_config["optimizer"], choices=["adamw","prodigy"])
            lr_scheduler = gr.Dropdown(label=get_text('lr_scheduler'), value=page_config["lr_scheduler"], 
                        choices=["linear", "cosine", "cosine_with_restarts", "polynomial","constant", "constant_with_warmup"])
            cosine_restarts = gr.Number(label=get_text('cosine_restarts'), value=page_config["cosine_restarts"], info=get_text('cosine_restarts_info'), minimum=1)
        with gr.Row():
            learning_rate = gr.Number(label=get_text('learning_rate'), value=page_config["learning_rate"], info=get_text('learning_rate_info'))
            lr_warmup_steps = gr.Number(label=get_text('lr_warmup_steps'), value=page_config["lr_warmup_steps"])
            seed = gr.Number(label=get_text('seed'), value=page_config["seed"])
        with gr.Row():
            blocks_to_swap = gr.Number(label=get_text('blocks_to_swap'), value=page_config["blocks_to_swap"], info=get_text('blocks_to_swap_info'))
            mask_dropout = gr.Number(label=get_text('mask_dropout'), value=page_config["mask_dropout"], info=get_text('mask_dropout_info'))
            reg_ratio = gr.Number(label=get_text('reg_ratio'), value=page_config["reg_ratio"], info=get_text('reg_ratio_info'))
            reg_timestep = gr.Number(label=get_text('reg_timestep'), value=page_config["reg_timestep"], info=get_text('reg_timestep_info'))
            
    misc_accordion = gr.Accordion(get_text('misc'))
    with misc_accordion:
        with gr.Row():
            num_train_epochs = gr.Number(label=get_text('num_train_epochs'), value=page_config["num_train_epochs"], info=get_text('num_train_epochs_info'))
            save_model_epochs = gr.Number(label=get_text('save_model_epochs'), value=page_config["save_model_epochs"], info=get_text('save_model_epochs_info'))
            validation_epochs = gr.Number(label=get_text('validation_epochs'), value=page_config["validation_epochs"], info=get_text('validation_epochs_info'))
        with gr.Row():
            skip_epoch = gr.Number(label=get_text('skip_epoch'), value=page_config["skip_epoch"], info=get_text('skip_epoch_info'))
            skip_step = gr.Number(label=get_text('skip_step'), value=page_config["skip_step"], info=get_text('skip_step_info'))
            validation_ratio = gr.Number(label=get_text('validation_ratio'), value=page_config["validation_ratio"], info=get_text('validation_ratio_info'))
            
        with gr.Row():
            recreate_cache = gr.Checkbox(label=get_text('recreate_cache'), value=page_config["recreate_cache"])
            caption_dropout = gr.Number(label=get_text('caption_dropout'), value=page_config["caption_dropout"], info=get_text('caption_dropout_info'), maximum=1, minimum=0)
            max_time_steps = gr.Number(label=get_text('max_time_steps'), value=page_config["max_time_steps"], info=get_text('max_time_steps_info'), maximum=1000, minimum=0)
        
        resolution_md = gr.Markdown(get_text('resolution_section'))
        with gr.Row():
            resolution = gr.Dropdown(label=get_text('resolution'), value=page_config["resolution"], choices=page_config["resolution_choices"])
    
    # è¾“å‡ºå’Œè¿è¡ŒæŒ‰é’®
    output = gr.Textbox(label=get_text('output_box'))
    run_btn = gr.Button(get_text('run_button'), variant="primary")
    
    # å®šä¹‰æ‰€æœ‰è¾“å…¥ç»„ä»¶åˆ—è¡¨
    inputs = [
        config_path,
        script,
        seed,
        # logging_dir,
        mixed_precision,
        report_to,
        lr_warmup_steps,
        output_dir,
        save_name,
        train_data_dir,
        optimizer,
        lr_scheduler,
        learning_rate,
        train_batch_size,
        repeats,
        gradient_accumulation_steps,
        num_train_epochs,
        save_model_epochs,
        validation_epochs,
        rank,
        skip_epoch,
        # break_epoch,
        skip_step,
        gradient_checkpointing,
        validation_ratio,
        pretrained_model_name_or_path,
        model_path,
        resume_from_checkpoint,
        # use_dora,
        recreate_cache,
        # vae_path,
        resolution,
        # use_debias,
        # snr_gamma,
        caption_dropout,
        cosine_restarts,
        max_time_steps,
        blocks_to_swap,
        mask_dropout,
        reg_ratio,
        reg_timestep
        # freeze_transformer_layers,
    ]
    
    # ç»‘å®šäº‹ä»¶å¤„ç†å™¨
    run_btn.click(fn=run, inputs=inputs, outputs=output, api_name="run")

    # ç»‘å®šåŠ è½½é…ç½®äº‹ä»¶ - åªä»è·¯å¾„è¯»å–
    load_config_btn.click(fn=load_config_from_file, inputs=[config_path], outputs=[
        config_path, script, seed, mixed_precision, report_to, lr_warmup_steps,
        output_dir, save_name, train_data_dir, optimizer, lr_scheduler, learning_rate,
        train_batch_size, repeats, gradient_accumulation_steps, num_train_epochs, 
        save_model_epochs, validation_epochs, rank, skip_epoch, skip_step, 
        gradient_checkpointing, validation_ratio, pretrained_model_name_or_path, 
        model_path, resume_from_checkpoint, recreate_cache, resolution,
        caption_dropout, cosine_restarts, max_time_steps, blocks_to_swap, 
        mask_dropout, reg_ratio, reg_timestep
    ])

    # ç»‘å®šä¸‹è½½é…ç½®äº‹ä»¶
    download_btn.click(
        fn=download_config_file_for_button,
        inputs=[config_path],
        outputs=[download_file]
    )

    # ç»‘å®šæ–‡ä»¶ä¸Šä¼ äº‹ä»¶ - ä¸Šä¼ åè‡ªåŠ¨æ›´æ–°è·¯å¾„
    config_file_upload.change(fn=handle_file, inputs=config_file_upload, outputs=config_path)

    # è¯­è¨€åˆ‡æ¢äº‹ä»¶å¤„ç† - æ›´æ–°æ‰€æœ‰ç»„ä»¶
    language_toggle_btn.click(
        fn=update_language_interface,
        inputs=[],
        outputs=[
            title_md, refresh_config_btn, language_toggle_btn, script, config_path, load_config_btn, download_btn, download_file,
            output_dir, save_name, pretrained_model_name_or_path, resume_from_checkpoint, 
            train_data_dir, model_path, report_to,
            rank, train_batch_size, repeats, gradient_accumulation_steps, mixed_precision, gradient_checkpointing,
            optimizer, lr_scheduler, cosine_restarts, learning_rate, lr_warmup_steps, seed,
            blocks_to_swap, mask_dropout, reg_ratio, reg_timestep,
            num_train_epochs, save_model_epochs, validation_epochs, skip_epoch, skip_step, validation_ratio,
            recreate_cache, caption_dropout, max_time_steps, resolution_md, resolution,
            output, run_btn
        ]
    )

# å¯åŠ¨ç•Œé¢
if __name__ == "__main__":
    demo.launch(
        server_port=6006,
    )